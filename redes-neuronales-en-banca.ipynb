{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes neuronales aplicadas a datos bancarios!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carga de datos **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cargamos los datos, importamos librerias y vemos que forma tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('Bank_registries.csv')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DespuÃ©s separamos las variables dependientes de la variable independiente a predecir (Exited). Ignoramos las columnas RowNumber, CustomerID y Surname porque no aportan valor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1       2   3  4         5  6  7  8          9\n",
       "0  619  France  Female  42  2       0.0  1  1  1  101348.88\n",
       "1  608   Spain  Female  41  1  83807.86  1  0  1  112542.58\n",
       "2  502  France  Female  42  8  159660.8  3  1  0  113931.57\n",
       "3  699  France  Female  39  1       0.0  2  0  0   93826.63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "pd.DataFrame(X[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Party time!! (o limpieza de datos, segun se mire...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos *malvadas* variables categoricas asÃ­ que aplicamos one hot encoding y Dummy encoding.\n",
    "\n",
    "Â¿QuÃ© mierda era eso?  \n",
    "    One Hot Enconding consiste en binarizar variables categoricas  \n",
    "    Dummy Encondig consiste en desdoblar una variable categorica en tantas columnas como niveles tenga, menos una.\n",
    "   \n",
    "![](https://sayingimages.com/wp-content/uploads/Wtf-Lol-meme.png)\n",
    "\n",
    "Para el caso de los paises, puesto que tenemos k niveles, creamos k-1 nuevas columnas, donde representaremos con un 1(True) o 0(False) la pertenencia de esa persona a ese pais. ejemplo:\n",
    "\n",
    "\n",
    "|  | Alemania | EspaÃ±a |\n",
    "|---------|----------|--------|\n",
    "| AlemÃ¡n | 1 | 0 |\n",
    "| EspaÃ±ol | 0 | 1 |\n",
    "| FrancÃ©s | 0 | 0 |  \n",
    "\n",
    "De esta manera, para 3 (k) niveles, representamos toda la informacion con 2(k-1) columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DespuÃ©s del Label Encoding:\n",
      "Forma de X: (10000, 10)\n",
      "Primeras 10 filas:\n",
      "[[619 0 0 42 2 0.0 1 1 1 101348.88]\n",
      " [608 2 0 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 0 0 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 0 0 39 1 0.0 2 0 0 93826.63]\n",
      " [850 2 0 43 2 125510.82 1 1 1 79084.1]\n",
      " [645 2 1 44 8 113755.78 2 1 0 149756.71]\n",
      " [822 0 1 50 7 0.0 2 1 1 10062.8]\n",
      " [376 1 0 29 4 115046.74 4 1 0 119346.88]\n",
      " [501 0 1 44 4 142051.07 2 0 1 74940.5]\n",
      " [684 0 1 27 2 134603.88 1 1 1 71725.73]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>6382.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24924.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>233</td>\n",
       "      <td>5014</td>\n",
       "      <td>5457</td>\n",
       "      <td>478</td>\n",
       "      <td>1048</td>\n",
       "      <td>3617.0</td>\n",
       "      <td>5084</td>\n",
       "      <td>7055</td>\n",
       "      <td>5151</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4        5      6      7      8  \\\n",
       "count   10000  10000  10000  10000  10000  10000.0  10000  10000  10000   \n",
       "unique    460      3      2     70     11   6382.0      4      2      2   \n",
       "top       850      0      1     37      2      0.0      1      1      1   \n",
       "freq      233   5014   5457    478   1048   3617.0   5084   7055   5151   \n",
       "\n",
       "               9  \n",
       "count   10000.00  \n",
       "unique   9999.00  \n",
       "top     24924.92  \n",
       "freq        2.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder transforma a nÃºmeros los niveles de la variable categÃ³rica. \n",
    "# OneHotEncoder desdobla en k-columnas binarias los k-niveles de cada variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Cargamos el modelo y transformamos los niveles categÃ³ricos a nÃºmeros consecutivos para (Geography y Gender)\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography (columna 1)\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender (columna 2)\n",
    "\n",
    "print(\"DespuÃ©s del Label Encoding:\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(\"Primeras 10 filas:\")\n",
    "print(X[0:10])\n",
    "\n",
    "# Verificamos los datos transformados\n",
    "pd.DataFrame(X).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma despuÃ©s de One-Hot Encoding: (10000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>460</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>6382.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>850</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24924.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7491.0</td>\n",
       "      <td>7523.0</td>\n",
       "      <td>233</td>\n",
       "      <td>5457</td>\n",
       "      <td>478</td>\n",
       "      <td>1048</td>\n",
       "      <td>3617.0</td>\n",
       "      <td>5084</td>\n",
       "      <td>7055</td>\n",
       "      <td>5151</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1      2      3      4      5        6      7      8   \\\n",
       "count   10000.0  10000.0  10000  10000  10000  10000  10000.0  10000  10000   \n",
       "unique      2.0      2.0    460      2     70     11   6382.0      4      2   \n",
       "top         0.0      0.0    850      1     37      2      0.0      1      1   \n",
       "freq     7491.0   7523.0    233   5457    478   1048   3617.0   5084   7055   \n",
       "\n",
       "           9         10  \n",
       "count   10000  10000.00  \n",
       "unique      2   9999.00  \n",
       "top         1  24924.92  \n",
       "freq     5151      2.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos Dummy Encoding, generando k-1 nuevas columnas para los k niveles de las variables categoricas\n",
    "# MÃ©todo moderno con ColumnTransformer (recomendado para scikit-learn >= 0.20)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Creamos el transformador para la columna 1 (Geography despuÃ©s del LabelEncoder)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Aplicamos la transformaciÃ³n\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Verificamos la forma de los datos\n",
    "print(f\"Forma despuÃ©s de One-Hot Encoding: {X.shape}\")\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ActualizaciÃ³n de Compatibilidad\n",
    "\n",
    "**Nota importante**: El cÃ³digo original usaba sintaxis antigua de scikit-learn y Keras. He actualizado el cÃ³digo para ser compatible con las versiones modernas:\n",
    "\n",
    "### Cambios realizados:\n",
    "\n",
    "1. **OneHotEncoder**: \n",
    "   - âŒ Antiguo: `OneHotEncoder(categorical_features=[1])`\n",
    "   - âœ… Nuevo: `ColumnTransformer` con `OneHotEncoder(drop='first')`\n",
    "\n",
    "2. **Keras/TensorFlow**:\n",
    "   - âŒ Antiguo: `from keras.models import Sequential`\n",
    "   - âœ… Nuevo: `from tensorflow.keras.models import Sequential`\n",
    "\n",
    "3. **ParÃ¡metros de Dense**:\n",
    "   - âŒ Antiguo: `output_dim`, `init`\n",
    "   - âœ… Nuevo: `units`, `kernel_initializer`\n",
    "\n",
    "4. **ParÃ¡metros de fit**:\n",
    "   - âŒ Antiguo: `nb_epoch`\n",
    "   - âœ… Nuevo: `epochs`\n",
    "\n",
    "Estos cambios mantienen la misma funcionalidad pero con la sintaxis moderna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por fin! seguimos modelando datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos en Train (80%) y test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos con Standard Scaler: Media = 0 y desviaciÃ³n standar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora si... Redes neuronales!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Sequetial inicializaremos la red y con Dense aÃ±adiremos las capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerÃ­as necesarias para redes neuronales\n",
    "# Usando TensorFlow 2.x (mÃ©todo moderno)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empecemos!\n",
    "![](https://memecrunch.com/meme/59A89/let-s-party/image.gif?w=499&c=1)\n",
    "\n",
    "A nuestra funcion le aÃ±adirÃ©mos capas(.add) con la funcion Dense. Pero... y los parametros?\n",
    "\n",
    "* Output_dim-->nÂº de nodos en la capa  \n",
    "* init--> inicializacion del descenso de gradiente estocÃ¡stico (se que lo sabes, pero por si necesitas recordar... [link](https://unipython.com/descenso-gradientes-estocastico-sgd/)) en este caso la distribuciÃ³n inicial de pesos de cada nodo sigue una variable aleatoria uniforme.  \n",
    "* input_dim--> es el numero de variables de entrada, el resto de capas lo heredan.  \n",
    "* Activation--> cada neurona tiene una funcion de activaciÃ³n que determina la *intensidad* (max. 1) con la que transmite su seÃ±al a la siguiente capa. las dos primeras capas usan la funcion [ReLU](https://es.wikipedia.org/wiki/Rectificador_(redes_neuronales) y la ultima una [sigmoide](https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide) para clasificar\n",
    "\n",
    "![](https://i.stack.imgur.com/bzQb3.png)\n",
    "\n",
    "![Definicion matemÃ¡tica](https://i.stack.imgur.com/VqOpE.jpg \"Math jiberish\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando importaciones...\n",
      "TensorFlow version: 2.19.0\n",
      "Keras integrado: 3.10.0\n",
      "âœ… Importaciones correctas\n",
      "NÃºmero de caracterÃ­sticas despuÃ©s del preprocessing: 11\n",
      "\n",
      "ğŸ—ï¸ Arquitectura del modelo:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\PyhtonIA\\DL Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚            \u001b[38;5;34m72\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚            \u001b[38;5;34m42\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚             \u001b[38;5;34m7\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> (484.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121\u001b[0m (484.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> (484.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121\u001b[0m (484.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verificamos que las importaciones estÃ©n disponibles\n",
    "try:\n",
    "    print(\"Verificando importaciones...\")\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Keras integrado: {tf.keras.__version__}\")\n",
    "    print(\"âœ… Importaciones correctas\")\n",
    "except NameError:\n",
    "    print(\"âŒ Error: Ejecuta primero la celda 18 (importaciones)\")\n",
    "    raise\n",
    "\n",
    "# Verificamos que X estÃ© disponible para determinar input_dim\n",
    "try:\n",
    "    print(f\"NÃºmero de caracterÃ­sticas despuÃ©s del preprocessing: {X.shape[1]}\")\n",
    "    input_features = X.shape[1]\n",
    "except NameError:\n",
    "    print(\"âŒ Error: Variable X no disponible. Ejecuta las celdas de preprocessing\")\n",
    "    raise\n",
    "\n",
    "# Inicializamos el clasificador (reinicializamos para evitar problemas)\n",
    "classifier = Sequential()\n",
    "\n",
    "# AÃ±adimos las capas a la red neuronal\n",
    "# Nota: 'output_dim' cambiÃ³ a 'units' e 'init' cambiÃ³ a 'kernel_initializer' en TensorFlow 2.x\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=input_features))\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Mostramos la arquitectura del modelo\n",
    "print(\"\\nğŸ—ï¸ Arquitectura del modelo:\")\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A compilar!! (esto no era python??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MÃ¡s argumentos!!  \n",
    "\n",
    "* Optimizer--> [Adam](https://data.sngular.com/es/art/60/adam-automated-discovery-and-analysis-machine) es el algoritmo de descenso de gradiente estocÃ¡stico que seleccionarÃ¡ los pesos Ã³ptimos de la red.  \n",
    "* Loss--> funcion de perdida a optimizar. En este caso es una clasificacion binaria por lo que... [binary_crossentropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) (si fueran mÃ¡s categorias usariamos [categorical_crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/))  \n",
    "* metrics-->Estamos muy arriba como para no incluir una metrica que nos diga cuÃ¡nto lo estÃ¡ petando nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train al night long!!\n",
    "\n",
    "![](https://survivalpioneer.com/wp-content/uploads/2018/12/Thomas-The-Train-Meme-16-200x300.jpg)\n",
    "\n",
    "Entrenamos la red con... mas parametros!!!  \n",
    "\n",
    "* Batch_size--> nÃºmero de observaciones que la red necesita entrenar antes de actualizar los pesos. \n",
    "* Epoch--> nÃºmero de iteraciones que realizaremos. No hay una regla especifica para escoger estos dos valores por lo que hay que hacerlo a prueba (esperabas ciencia verdad??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Entrenamiento rÃ¡pido de la red neuronal...\n",
      "==================================================\n",
      "ğŸ” Verificando variables...\n",
      "   X_train: (8000, 11)\n",
      "   y_train: (8000,)\n",
      "   X_test: (2000, 11)\n",
      "   y_test: (2000,)\n",
      "âœ… Variables disponibles\n",
      "\n",
      "ğŸ—ï¸ Modelo: 121 parÃ¡metros\n",
      "âœ… Modelo disponible\n",
      "\n",
      "ğŸš€ Iniciando entrenamiento rÃ¡pido...\n",
      "ğŸ“Š Datos: 8000 muestras, 11 caracterÃ­sticas\n",
      "â±ï¸ Entrenamiento optimizado (50 Ã©pocas)...\n",
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8665 - loss: 0.3318 - val_accuracy: 0.8600 - val_loss: 0.3361\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8665 - loss: 0.3318 - val_accuracy: 0.8600 - val_loss: 0.3361\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3349 - val_accuracy: 0.8587 - val_loss: 0.3352\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3349 - val_accuracy: 0.8587 - val_loss: 0.3352\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.3285 - val_accuracy: 0.8600 - val_loss: 0.3353\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.3285 - val_accuracy: 0.8600 - val_loss: 0.3353\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8664 - loss: 0.3296 - val_accuracy: 0.8587 - val_loss: 0.3371\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8664 - loss: 0.3296 - val_accuracy: 0.8587 - val_loss: 0.3371\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8532 - loss: 0.3504 - val_accuracy: 0.8575 - val_loss: 0.3351\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8532 - loss: 0.3504 - val_accuracy: 0.8575 - val_loss: 0.3351\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8687 - loss: 0.3245 - val_accuracy: 0.8550 - val_loss: 0.3357\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8687 - loss: 0.3245 - val_accuracy: 0.8550 - val_loss: 0.3357\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.3444 - val_accuracy: 0.8575 - val_loss: 0.3348\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.3444 - val_accuracy: 0.8575 - val_loss: 0.3348\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3443 - val_accuracy: 0.8600 - val_loss: 0.3357\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3443 - val_accuracy: 0.8600 - val_loss: 0.3357\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3389 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3389 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3333 - val_accuracy: 0.8587 - val_loss: 0.3354\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3333 - val_accuracy: 0.8587 - val_loss: 0.3354\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3543 - val_accuracy: 0.8550 - val_loss: 0.3344\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3543 - val_accuracy: 0.8550 - val_loss: 0.3344\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8575 - loss: 0.3402 - val_accuracy: 0.8625 - val_loss: 0.3364\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8575 - loss: 0.3402 - val_accuracy: 0.8625 - val_loss: 0.3364\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3420 - val_accuracy: 0.8575 - val_loss: 0.3352\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3420 - val_accuracy: 0.8575 - val_loss: 0.3352\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3356 - val_accuracy: 0.8600 - val_loss: 0.3350\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3356 - val_accuracy: 0.8600 - val_loss: 0.3350\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3409 - val_accuracy: 0.8575 - val_loss: 0.3346\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3409 - val_accuracy: 0.8575 - val_loss: 0.3346\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3353 - val_accuracy: 0.8587 - val_loss: 0.3323\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3353 - val_accuracy: 0.8587 - val_loss: 0.3323\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.3433 - val_accuracy: 0.8625 - val_loss: 0.3353\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.3433 - val_accuracy: 0.8625 - val_loss: 0.3353\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3339 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3339 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3319 - val_accuracy: 0.8612 - val_loss: 0.3354\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3319 - val_accuracy: 0.8612 - val_loss: 0.3354\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3356 - val_accuracy: 0.8575 - val_loss: 0.3341\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3356 - val_accuracy: 0.8575 - val_loss: 0.3341\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3284 - val_accuracy: 0.8587 - val_loss: 0.3344\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3284 - val_accuracy: 0.8587 - val_loss: 0.3344\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.3363 - val_accuracy: 0.8537 - val_loss: 0.3327\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.3363 - val_accuracy: 0.8537 - val_loss: 0.3327\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3376 - val_accuracy: 0.8637 - val_loss: 0.3333\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3376 - val_accuracy: 0.8637 - val_loss: 0.3333\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3352 - val_accuracy: 0.8587 - val_loss: 0.3330\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3352 - val_accuracy: 0.8587 - val_loss: 0.3330\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.3368 - val_accuracy: 0.8650 - val_loss: 0.3374\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.3368 - val_accuracy: 0.8650 - val_loss: 0.3374\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.3202 - val_accuracy: 0.8587 - val_loss: 0.3347\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.3202 - val_accuracy: 0.8587 - val_loss: 0.3347\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8675 - loss: 0.3330 - val_accuracy: 0.8600 - val_loss: 0.3343\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8675 - loss: 0.3330 - val_accuracy: 0.8600 - val_loss: 0.3343\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8680 - loss: 0.3262 - val_accuracy: 0.8612 - val_loss: 0.3326\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8680 - loss: 0.3262 - val_accuracy: 0.8612 - val_loss: 0.3326\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3273 - val_accuracy: 0.8637 - val_loss: 0.3373\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3273 - val_accuracy: 0.8637 - val_loss: 0.3373\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3440 - val_accuracy: 0.8612 - val_loss: 0.3335\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3440 - val_accuracy: 0.8612 - val_loss: 0.3335\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8610 - loss: 0.3324 - val_accuracy: 0.8600 - val_loss: 0.3341\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8610 - loss: 0.3324 - val_accuracy: 0.8600 - val_loss: 0.3341\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8648 - loss: 0.3356 - val_accuracy: 0.8625 - val_loss: 0.3318\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8648 - loss: 0.3356 - val_accuracy: 0.8625 - val_loss: 0.3318\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3437 - val_accuracy: 0.8637 - val_loss: 0.3316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3437 - val_accuracy: 0.8637 - val_loss: 0.3316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8673 - loss: 0.3313 - val_accuracy: 0.8587 - val_loss: 0.3361\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8673 - loss: 0.3313 - val_accuracy: 0.8587 - val_loss: 0.3361\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8523 - loss: 0.3513 - val_accuracy: 0.8600 - val_loss: 0.3319\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8523 - loss: 0.3513 - val_accuracy: 0.8600 - val_loss: 0.3319\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.3193 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.3193 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.3328 - val_accuracy: 0.8625 - val_loss: 0.3333\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.3328 - val_accuracy: 0.8625 - val_loss: 0.3333\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8611 - loss: 0.3396 - val_accuracy: 0.8625 - val_loss: 0.3338\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8611 - loss: 0.3396 - val_accuracy: 0.8625 - val_loss: 0.3338\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3461 - val_accuracy: 0.8600 - val_loss: 0.3324\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3461 - val_accuracy: 0.8600 - val_loss: 0.3324\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8642 - loss: 0.3376 - val_accuracy: 0.8612 - val_loss: 0.3333\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8642 - loss: 0.3376 - val_accuracy: 0.8612 - val_loss: 0.3333\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.3400 - val_accuracy: 0.8612 - val_loss: 0.3358\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.3400 - val_accuracy: 0.8612 - val_loss: 0.3358\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8580 - loss: 0.3391 - val_accuracy: 0.8575 - val_loss: 0.3337\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8580 - loss: 0.3391 - val_accuracy: 0.8575 - val_loss: 0.3337\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3416 - val_accuracy: 0.8637 - val_loss: 0.3328\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3416 - val_accuracy: 0.8637 - val_loss: 0.3328\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3470 - val_accuracy: 0.8587 - val_loss: 0.3305\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3470 - val_accuracy: 0.8587 - val_loss: 0.3305\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3349 - val_accuracy: 0.8600 - val_loss: 0.3347\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3349 - val_accuracy: 0.8600 - val_loss: 0.3347\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3432 - val_accuracy: 0.8612 - val_loss: 0.3323\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3432 - val_accuracy: 0.8612 - val_loss: 0.3323\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3355 - val_accuracy: 0.8575 - val_loss: 0.3327\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3355 - val_accuracy: 0.8575 - val_loss: 0.3327\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3298 - val_accuracy: 0.8587 - val_loss: 0.3341\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3298 - val_accuracy: 0.8587 - val_loss: 0.3341\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3351 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3351 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8616 - loss: 0.3433 - val_accuracy: 0.8575 - val_loss: 0.3318\n",
      "\n",
      "ğŸ“Š RESULTADOS DEL ENTRENAMIENTO:\n",
      "   â€¢ PÃ©rdida final: 0.3371\n",
      "   â€¢ PrecisiÃ³n final: 0.8615\n",
      "   â€¢ PÃ©rdida validaciÃ³n: 0.3318\n",
      "   â€¢ PrecisiÃ³n validaciÃ³n: 0.8575\n",
      "\n",
      "ğŸ‰ Â¡ENTRENAMIENTO RÃPIDO COMPLETADO!\n",
      "ğŸ’¡ Para entrenamiento mÃ¡s largo, cambiar epochs=50 por epochs=500\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8616 - loss: 0.3433 - val_accuracy: 0.8575 - val_loss: 0.3318\n",
      "\n",
      "ğŸ“Š RESULTADOS DEL ENTRENAMIENTO:\n",
      "   â€¢ PÃ©rdida final: 0.3371\n",
      "   â€¢ PrecisiÃ³n final: 0.8615\n",
      "   â€¢ PÃ©rdida validaciÃ³n: 0.3318\n",
      "   â€¢ PrecisiÃ³n validaciÃ³n: 0.8575\n",
      "\n",
      "ğŸ‰ Â¡ENTRENAMIENTO RÃPIDO COMPLETADO!\n",
      "ğŸ’¡ Para entrenamiento mÃ¡s largo, cambiar epochs=50 por epochs=500\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ENTRENAMIENTO RÃPIDO DE LA RED NEURONAL\n",
    "print(\"ğŸš€ Entrenamiento rÃ¡pido de la red neuronal...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar que las variables estÃ©n disponibles\n",
    "try:\n",
    "    print(\"ğŸ” Verificando variables...\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    print(f\"   y_test: {y_test.shape}\")\n",
    "    print(\"âœ… Variables disponibles\")\n",
    "except NameError:\n",
    "    print(\"âŒ Variables no disponibles\")\n",
    "    print(\"ğŸ”„ Ejecuta primero la celda de 'CONFIGURACIÃ“N COMPLETA AUTOMÃTICA'\")\n",
    "    raise\n",
    "\n",
    "# Verificar modelo\n",
    "try:\n",
    "    print(f\"\\nğŸ—ï¸ Modelo: {classifier.count_params()} parÃ¡metros\")\n",
    "    print(\"âœ… Modelo disponible\")\n",
    "except NameError:\n",
    "    print(\"âŒ Modelo no disponible\")\n",
    "    print(\"ğŸ”„ Ejecuta primero la celda de 'CONFIGURACIÃ“N COMPLETA AUTOMÃTICA'\")\n",
    "    raise\n",
    "\n",
    "# ENTRENAR CON PARÃMETROS OPTIMIZADOS\n",
    "print(\"\\nğŸš€ Iniciando entrenamiento rÃ¡pido...\")\n",
    "print(f\"ğŸ“Š Datos: {X_train.shape[0]} muestras, {X_train.shape[1]} caracterÃ­sticas\")\n",
    "print(\"â±ï¸ Entrenamiento optimizado (50 Ã©pocas)...\")\n",
    "\n",
    "# Entrenar la red neuronal con menos Ã©pocas para mayor velocidad\n",
    "history = classifier.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=50,  # Reducido de 500 a 50 para mayor velocidad\n",
    "    verbose=1,\n",
    "    validation_split=0.1  # 10% para validaciÃ³n\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nğŸ“Š RESULTADOS DEL ENTRENAMIENTO:\")\n",
    "print(f\"   â€¢ PÃ©rdida final: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"   â€¢ PrecisiÃ³n final: {history.history['accuracy'][-1]:.4f}\")\n",
    "if 'val_loss' in history.history:\n",
    "    print(f\"   â€¢ PÃ©rdida validaciÃ³n: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   â€¢ PrecisiÃ³n validaciÃ³n: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Â¡ENTRENAMIENTO RÃPIDO COMPLETADO!\")\n",
    "print(\"ğŸ’¡ Para entrenamiento mÃ¡s largo, cambiar epochs=50 por epochs=500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ ConfiguraciÃ³n completa automÃ¡tica del entorno...\n",
      "============================================================\n",
      "âœ… Importaciones completadas\n",
      "\n",
      "ğŸ“Š 1. Cargando datos...\n",
      "   Dataset shape: (10000, 14)\n",
      "\n",
      "ğŸ”§ 2. Separando variables...\n",
      "   X shape: (10000, 10), y shape: (10000,)\n",
      "\n",
      "ğŸ·ï¸ 3. Label Encoding...\n",
      "   Label Encoding completado\n",
      "\n",
      "ğŸ”„ 4. One-Hot Encoding...\n",
      "   One-Hot Encoding completado: (10000, 11)\n",
      "\n",
      "âœ‚ï¸ 5. Train/Test Split...\n",
      "   Train: (8000, 11), Test: (2000, 11)\n",
      "\n",
      "ğŸ“ 6. NormalizaciÃ³n...\n",
      "   NormalizaciÃ³n completada\n",
      "\n",
      "ğŸ—ï¸ 7. Creando modelo...\n",
      "\n",
      "âš™ï¸ 8. Compilando modelo...\n",
      "\n",
      "ğŸ‰ Â¡CONFIGURACIÃ“N COMPLETA EXITOSA!\n",
      "\n",
      "ğŸ“Š RESUMEN FINAL:\n",
      "   â€¢ Dataset: 10000 registros\n",
      "   â€¢ CaracterÃ­sticas: 11\n",
      "   â€¢ Entrenamiento: 8000 muestras\n",
      "   â€¢ Prueba: 2000 muestras\n",
      "   â€¢ Modelo: 121 parÃ¡metros\n",
      "\n",
      "âœ… TODO LISTO PARA ENTRENAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\PyhtonIA\\DL Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ CONFIGURACIÃ“N COMPLETA AUTOMÃTICA\n",
    "print(\"ğŸ› ï¸ ConfiguraciÃ³n completa automÃ¡tica del entorno...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"âœ… Importaciones completadas\")\n",
    "\n",
    "# FunciÃ³n completa de configuraciÃ³n\n",
    "def configuracion_completa():\n",
    "    global dataset, X, y, X_train, X_test, y_train, y_test, sc, classifier\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    print(\"\\nğŸ“Š 1. Cargando datos...\")\n",
    "    dataset = pd.read_csv('Bank_registries.csv')\n",
    "    print(f\"   Dataset shape: {dataset.shape}\")\n",
    "    \n",
    "    # 2. Separar variables\n",
    "    print(\"\\nğŸ”§ 2. Separando variables...\")\n",
    "    X = dataset.iloc[:, 3:13].values\n",
    "    y = dataset.iloc[:, 13].values\n",
    "    print(f\"   X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # 3. Label Encoding\n",
    "    print(\"\\nğŸ·ï¸ 3. Label Encoding...\")\n",
    "    labelencoder_X_1 = LabelEncoder()\n",
    "    X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography\n",
    "    labelencoder_X_2 = LabelEncoder()\n",
    "    X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender\n",
    "    print(\"   Label Encoding completado\")\n",
    "    \n",
    "    # 4. One-Hot Encoding\n",
    "    print(\"\\nğŸ”„ 4. One-Hot Encoding...\")\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    X = ct.fit_transform(X)\n",
    "    print(f\"   One-Hot Encoding completado: {X.shape}\")\n",
    "    \n",
    "    # 5. Train/Test Split\n",
    "    print(\"\\nâœ‚ï¸ 5. Train/Test Split...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 6. NormalizaciÃ³n\n",
    "    print(\"\\nğŸ“ 6. NormalizaciÃ³n...\")\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print(\"   NormalizaciÃ³n completada\")\n",
    "    \n",
    "    # 7. Crear modelo\n",
    "    print(\"\\nğŸ—ï¸ 7. Creando modelo...\")\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X.shape[1]))\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    # 8. Compilar modelo\n",
    "    print(\"\\nâš™ï¸ 8. Compilando modelo...\")\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nğŸ‰ Â¡CONFIGURACIÃ“N COMPLETA EXITOSA!\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Ejecutar configuraciÃ³n completa\n",
    "X_train, X_test, y_train, y_test = configuracion_completa()\n",
    "\n",
    "# Mostrar resumen final\n",
    "print(\"\\nğŸ“Š RESUMEN FINAL:\")\n",
    "print(f\"   â€¢ Dataset: {dataset.shape[0]} registros\")\n",
    "print(f\"   â€¢ CaracterÃ­sticas: {X_train.shape[1]}\")\n",
    "print(f\"   â€¢ Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"   â€¢ Prueba: {X_test.shape[0]} muestras\")\n",
    "print(f\"   â€¢ Modelo: {classifier.count_params()} parÃ¡metros\")\n",
    "print(\"\\nâœ… TODO LISTO PARA ENTRENAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” VERIFICACIÃ“N COMPLETA DEL FLUJO DE DATOS\n",
    "print(\"ğŸ” VerificaciÃ³n completa del flujo de datos:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Lista de variables necesarias\n",
    "required_vars = {\n",
    "    'dataset': 'DataFrame original',\n",
    "    'X': 'Variables independientes procesadas',\n",
    "    'y': 'Variable dependiente',\n",
    "    'X_train': 'Datos de entrenamiento (X)',\n",
    "    'y_train': 'Etiquetas de entrenamiento (y)',\n",
    "    'X_test': 'Datos de prueba (X)',\n",
    "    'y_test': 'Etiquetas de prueba (y)',\n",
    "    'sc': 'StandardScaler',\n",
    "    'classifier': 'Modelo de red neuronal'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var_name, description in required_vars.items():\n",
    "    try:\n",
    "        var_value = locals()[var_name]\n",
    "        if hasattr(var_value, 'shape'):\n",
    "            print(f\"âœ… {var_name}: {description} - Shape: {var_value.shape}\")\n",
    "        else:\n",
    "            print(f\"âœ… {var_name}: {description} - Disponible\")\n",
    "    except KeyError:\n",
    "        print(f\"âŒ {var_name}: {description} - NO DISPONIBLE\")\n",
    "        missing_vars.append(var_name)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\nğŸš¨ FALTAN VARIABLES: {', '.join(missing_vars)}\")\n",
    "    print(\"\\nğŸ“‹ EJECUTA ESTAS CELDAS EN ORDEN:\")\n",
    "    print(\"   1. Celda 4: Carga de datos\")\n",
    "    print(\"   2. Celda 6: SeparaciÃ³n X, y\")\n",
    "    print(\"   3. Celda 9: Label Encoding\")\n",
    "    print(\"   4. Celda 10: One-Hot Encoding\")\n",
    "    print(\"   5. Celda 13: Train/Test Split\")\n",
    "    print(\"   6. Celda 15: NormalizaciÃ³n\")\n",
    "    print(\"   7. Celda 18: Importar TensorFlow\")\n",
    "    print(\"   8. Celda 19: Crear modelo\")\n",
    "    print(\"   9. Celda 21: AÃ±adir capas\")\n",
    "    print(\"   10. Celda 24: Compilar modelo\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ Â¡TODOS LOS DATOS ESTÃN LISTOS PARA ENTRENAR!\")\n",
    "    print(f\"ğŸ“Š Total de muestras de entrenamiento: {len(X_train)}\")\n",
    "    print(f\"ğŸ“Š Total de caracterÃ­sticas: {X_train.shape[1]}\")\n",
    "    print(f\"ğŸ¯ DistribuciÃ³n de clases en y_train: {np.bincount(y_train.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ EJECUCIÃ“N AUTOMÃTICA DEL FLUJO DE DATOS\n",
    "# Esta celda ejecuta automÃ¡ticamente todo el preprocessing si faltan variables\n",
    "\n",
    "print(\"ğŸš€ Ejecutando flujo completo de datos...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Paso 1: Verificar si existe el dataset\n",
    "try:\n",
    "    print(\"ğŸ“Š Verificando dataset...\")\n",
    "    dataset.head()\n",
    "    print(\"âœ… Dataset disponible\")\n",
    "except NameError:\n",
    "    print(\"ğŸ“¥ Cargando dataset...\")\n",
    "    dataset = pd.read_csv('Bank_registries.csv')\n",
    "    print(f\"âœ… Dataset cargado: {dataset.shape}\")\n",
    "\n",
    "# Paso 2: Separar variables X e y\n",
    "print(\"\\nğŸ”§ Separando variables...\")\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print(f\"âœ… X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Paso 3: Label Encoding\n",
    "print(\"\\nğŸ·ï¸ Aplicando Label Encoding...\")\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender\n",
    "print(\"âœ… Label Encoding completado\")\n",
    "\n",
    "# Paso 4: One-Hot Encoding\n",
    "print(\"\\nğŸ”„ Aplicando One-Hot Encoding...\")\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X = ct.fit_transform(X)\n",
    "print(f\"âœ… One-Hot Encoding completado: {X.shape}\")\n",
    "\n",
    "# Paso 5: Train/Test Split\n",
    "print(\"\\nâœ‚ï¸ Dividiendo datos...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"âœ… Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Paso 6: NormalizaciÃ³n\n",
    "print(\"\\nğŸ“ Normalizando datos...\")\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(\"âœ… NormalizaciÃ³n completada\")\n",
    "\n",
    "print(\"\\nğŸ‰ Â¡FLUJO DE DATOS COMPLETADO!\")\n",
    "print(f\"ğŸ“Š Datos listos para entrenar: {X_train.shape[0]} muestras, {X_train.shape[1]} caracterÃ­sticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has visto como subia ese accuracy?? mmmm... acuuuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A predecir!\n",
    "\n",
    "separamos las predicciones a un valor u otro con 0.5 como punto de corte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we good?\n",
    "\n",
    "Calculamos la matriz de confusion para ver quÃ© tal nos ha ido:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1542,   78],\n",
       "       [ 195,  185]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8635\n"
     ]
    }
   ],
   "source": [
    "good = (cm[0][0] + cm[1][1])/np.sum(cm)\n",
    "print (good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.makeameme.org/created/we-good-zgv5sb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente no es un accuracy Ã©pico, pero tampoco nos hemos matado a trabajar.\n",
    "Recuerda, tienes mÃ¡s camino por delante que por detras para profundizar en esto!!\n",
    "\n",
    "(pd: cualquier duda, contacta! :D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ENTRENAMIENTO AVANZADO CON EARLY STOPPING (OPCIONAL)\n",
    "print(\"ğŸ¯ Entrenamiento avanzado con early stopping...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Importar callbacks para early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Verificar que las variables estÃ©n disponibles\n",
    "try:\n",
    "    print(\"ğŸ” Verificando variables...\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(\"âœ… Variables disponibles\")\n",
    "except NameError:\n",
    "    print(\"âŒ Variables no disponibles\")\n",
    "    print(\"ğŸ”„ Ejecuta primero la celda de 'CONFIGURACIÃ“N COMPLETA AUTOMÃTICA'\")\n",
    "    raise\n",
    "\n",
    "# Recrear el modelo para entrenamiento limpio\n",
    "print(\"\\nğŸ—ï¸ Recreando modelo para entrenamiento avanzado...\")\n",
    "classifier_advanced = Sequential()\n",
    "classifier_advanced.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "classifier_advanced.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "classifier_advanced.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compilar modelo\n",
    "classifier_advanced.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Configurar callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ENTRENAR CON EARLY STOPPING\n",
    "print(\"\\nğŸš€ Iniciando entrenamiento avanzado...\")\n",
    "print(\"â±ï¸ Se detendrÃ¡ automÃ¡ticamente cuando deje de mejorar...\")\n",
    "\n",
    "history_advanced = classifier_advanced.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=500,  # MÃ¡ximo 500, pero se detendrÃ¡ antes si no mejora\n",
    "    verbose=1,\n",
    "    validation_split=0.2,  # 20% para validaciÃ³n\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nğŸ“Š RESULTADOS DEL ENTRENAMIENTO AVANZADO:\")\n",
    "print(f\"   â€¢ Ã‰pocas ejecutadas: {len(history_advanced.history['loss'])}\")\n",
    "print(f\"   â€¢ PÃ©rdida final: {history_advanced.history['loss'][-1]:.4f}\")\n",
    "print(f\"   â€¢ PrecisiÃ³n final: {history_advanced.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   â€¢ Mejor pÃ©rdida validaciÃ³n: {min(history_advanced.history['val_loss']):.4f}\")\n",
    "print(f\"   â€¢ Mejor precisiÃ³n validaciÃ³n: {max(history_advanced.history['val_accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Â¡ENTRENAMIENTO AVANZADO COMPLETADO!\")\n",
    "print(\"ğŸ’¡ El modelo se detuvo automÃ¡ticamente al alcanzar el mejor rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ SoluciÃ³n del Error NameError\n",
    "\n",
    "### âœ… Problema Resuelto\n",
    "\n",
    "**Error original**: `NameError: name 'X_train' is not defined`\n",
    "\n",
    "**Causa**: Las variables `X_train`, `X_test`, `y_train`, `y_test` no estaban definidas porque:\n",
    "- Las celdas de preprocessing no se ejecutaron en orden\n",
    "- El kernel se reiniciÃ³ y se perdiÃ³ el estado\n",
    "- Se saltaron celdas crÃ­ticas como train_test_split\n",
    "\n",
    "### ğŸ¯ SoluciÃ³n Implementada\n",
    "\n",
    "1. **ConfiguraciÃ³n AutomÃ¡tica**: Ejecutamos la celda de \"CONFIGURACIÃ“N COMPLETA AUTOMÃTICA\" que:\n",
    "   - Carga los datos\n",
    "   - Realiza todo el preprocessing\n",
    "   - Crea las variables necesarias\n",
    "   - Prepara el modelo\n",
    "\n",
    "2. **VerificaciÃ³n Robusta**: Agregamos checks que verifican la existencia de variables antes de usarlas\n",
    "\n",
    "3. **Entrenamiento Optimizado**: Creamos dos versiones:\n",
    "   - **RÃ¡pida**: 50 Ã©pocas para pruebas rÃ¡pidas\n",
    "   - **Avanzada**: Con early stopping para mejor rendimiento\n",
    "\n",
    "### ğŸ’¡ Mejores PrÃ¡cticas\n",
    "\n",
    "- **Siempre ejecutar** la celda de configuraciÃ³n automÃ¡tica primero\n",
    "- **Verificar variables** antes de operaciones crÃ­ticas\n",
    "- **Usar mensajes informativos** para diagnosticar problemas\n",
    "- **Implementar early stopping** para entrenamiento eficiente\n",
    "\n",
    "### ğŸ“Š Resultados\n",
    "\n",
    "El modelo ahora entrena correctamente y produce:\n",
    "- **Accuracy**: ~80-85% (tÃ­pico para este dataset)\n",
    "- **Loss**: Decrece progresivamente durante el entrenamiento\n",
    "- **ValidaciÃ³n**: Monitorea el overfitting\n",
    "\n",
    "Â¡El error NameError ha sido completamente resuelto! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
