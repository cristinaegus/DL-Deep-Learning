{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes neuronales aplicadas a datos bancarios!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Carga de datos **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cargamos los datos, importamos librerias y vemos que forma tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('Bank_registries.csv')\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después separamos las variables dependientes de la variable independiente a predecir (Exited). Ignoramos las columnas RowNumber, CustomerID y Surname porque no aportan valor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1       2   3  4         5  6  7  8          9\n",
       "0  619  France  Female  42  2       0.0  1  1  1  101348.88\n",
       "1  608   Spain  Female  41  1  83807.86  1  0  1  112542.58\n",
       "2  502  France  Female  42  8  159660.8  3  1  0  113931.57\n",
       "3  699  France  Female  39  1       0.0  2  0  0   93826.63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "pd.DataFrame(X[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Party time!! (o limpieza de datos, segun se mire...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos *malvadas* variables categoricas así que aplicamos one hot encoding y Dummy encoding.\n",
    "\n",
    "¿Qué mierda era eso?  \n",
    "    One Hot Enconding consiste en binarizar variables categoricas  \n",
    "    Dummy Encondig consiste en desdoblar una variable categorica en tantas columnas como niveles tenga, menos una.\n",
    "   \n",
    "![](https://sayingimages.com/wp-content/uploads/Wtf-Lol-meme.png)\n",
    "\n",
    "Para el caso de los paises, puesto que tenemos k niveles, creamos k-1 nuevas columnas, donde representaremos con un 1(True) o 0(False) la pertenencia de esa persona a ese pais. ejemplo:\n",
    "\n",
    "\n",
    "|  | Alemania | España |\n",
    "|---------|----------|--------|\n",
    "| Alemán | 1 | 0 |\n",
    "| Español | 0 | 1 |\n",
    "| Francés | 0 | 0 |  \n",
    "\n",
    "De esta manera, para 3 (k) niveles, representamos toda la informacion con 2(k-1) columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después del Label Encoding:\n",
      "Forma de X: (10000, 10)\n",
      "Primeras 10 filas:\n",
      "[[619 0 0 42 2 0.0 1 1 1 101348.88]\n",
      " [608 2 0 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 0 0 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 0 0 39 1 0.0 2 0 0 93826.63]\n",
      " [850 2 0 43 2 125510.82 1 1 1 79084.1]\n",
      " [645 2 1 44 8 113755.78 2 1 0 149756.71]\n",
      " [822 0 1 50 7 0.0 2 1 1 10062.8]\n",
      " [376 1 0 29 4 115046.74 4 1 0 119346.88]\n",
      " [501 0 1 44 4 142051.07 2 0 1 74940.5]\n",
      " [684 0 1 27 2 134603.88 1 1 1 71725.73]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>6382.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24924.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>233</td>\n",
       "      <td>5014</td>\n",
       "      <td>5457</td>\n",
       "      <td>478</td>\n",
       "      <td>1048</td>\n",
       "      <td>3617.0</td>\n",
       "      <td>5084</td>\n",
       "      <td>7055</td>\n",
       "      <td>5151</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4        5      6      7      8  \\\n",
       "count   10000  10000  10000  10000  10000  10000.0  10000  10000  10000   \n",
       "unique    460      3      2     70     11   6382.0      4      2      2   \n",
       "top       850      0      1     37      2      0.0      1      1      1   \n",
       "freq      233   5014   5457    478   1048   3617.0   5084   7055   5151   \n",
       "\n",
       "               9  \n",
       "count   10000.00  \n",
       "unique   9999.00  \n",
       "top     24924.92  \n",
       "freq        2.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder transforma a números los niveles de la variable categórica. \n",
    "# OneHotEncoder desdobla en k-columnas binarias los k-niveles de cada variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Cargamos el modelo y transformamos los niveles categóricos a números consecutivos para (Geography y Gender)\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography (columna 1)\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender (columna 2)\n",
    "\n",
    "print(\"Después del Label Encoding:\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(\"Primeras 10 filas:\")\n",
    "print(X[0:10])\n",
    "\n",
    "# Verificamos los datos transformados\n",
    "pd.DataFrame(X).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma después de One-Hot Encoding: (10000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>460</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>6382.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>850</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24924.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7491.0</td>\n",
       "      <td>7523.0</td>\n",
       "      <td>233</td>\n",
       "      <td>5457</td>\n",
       "      <td>478</td>\n",
       "      <td>1048</td>\n",
       "      <td>3617.0</td>\n",
       "      <td>5084</td>\n",
       "      <td>7055</td>\n",
       "      <td>5151</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1      2      3      4      5        6      7      8   \\\n",
       "count   10000.0  10000.0  10000  10000  10000  10000  10000.0  10000  10000   \n",
       "unique      2.0      2.0    460      2     70     11   6382.0      4      2   \n",
       "top         0.0      0.0    850      1     37      2      0.0      1      1   \n",
       "freq     7491.0   7523.0    233   5457    478   1048   3617.0   5084   7055   \n",
       "\n",
       "           9         10  \n",
       "count   10000  10000.00  \n",
       "unique      2   9999.00  \n",
       "top         1  24924.92  \n",
       "freq     5151      2.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos Dummy Encoding, generando k-1 nuevas columnas para los k niveles de las variables categoricas\n",
    "# Método moderno con ColumnTransformer (recomendado para scikit-learn >= 0.20)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Creamos el transformador para la columna 1 (Geography después del LabelEncoder)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Aplicamos la transformación\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Verificamos la forma de los datos\n",
    "print(f\"Forma después de One-Hot Encoding: {X.shape}\")\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Actualización de Compatibilidad\n",
    "\n",
    "**Nota importante**: El código original usaba sintaxis antigua de scikit-learn y Keras. He actualizado el código para ser compatible con las versiones modernas:\n",
    "\n",
    "### Cambios realizados:\n",
    "\n",
    "1. **OneHotEncoder**: \n",
    "   - ❌ Antiguo: `OneHotEncoder(categorical_features=[1])`\n",
    "   - ✅ Nuevo: `ColumnTransformer` con `OneHotEncoder(drop='first')`\n",
    "\n",
    "2. **Keras/TensorFlow**:\n",
    "   - ❌ Antiguo: `from keras.models import Sequential`\n",
    "   - ✅ Nuevo: `from tensorflow.keras.models import Sequential`\n",
    "\n",
    "3. **Parámetros de Dense**:\n",
    "   - ❌ Antiguo: `output_dim`, `init`\n",
    "   - ✅ Nuevo: `units`, `kernel_initializer`\n",
    "\n",
    "4. **Parámetros de fit**:\n",
    "   - ❌ Antiguo: `nb_epoch`\n",
    "   - ✅ Nuevo: `epochs`\n",
    "\n",
    "Estos cambios mantienen la misma funcionalidad pero con la sintaxis moderna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por fin! seguimos modelando datos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos en Train (80%) y test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos con Standard Scaler: Media = 0 y desviación standar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora si... Redes neuronales!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Sequetial inicializaremos la red y con Dense añadiremos las capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para redes neuronales\n",
    "# Usando TensorFlow 2.x (método moderno)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empecemos!\n",
    "![](https://memecrunch.com/meme/59A89/let-s-party/image.gif?w=499&c=1)\n",
    "\n",
    "A nuestra funcion le añadirémos capas(.add) con la funcion Dense. Pero... y los parametros?\n",
    "\n",
    "* Output_dim-->nº de nodos en la capa  \n",
    "* init--> inicializacion del descenso de gradiente estocástico (se que lo sabes, pero por si necesitas recordar... [link](https://unipython.com/descenso-gradientes-estocastico-sgd/)) en este caso la distribución inicial de pesos de cada nodo sigue una variable aleatoria uniforme.  \n",
    "* input_dim--> es el numero de variables de entrada, el resto de capas lo heredan.  \n",
    "* Activation--> cada neurona tiene una funcion de activación que determina la *intensidad* (max. 1) con la que transmite su señal a la siguiente capa. las dos primeras capas usan la funcion [ReLU](https://es.wikipedia.org/wiki/Rectificador_(redes_neuronales) y la ultima una [sigmoide](https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide) para clasificar\n",
    "\n",
    "![](https://i.stack.imgur.com/bzQb3.png)\n",
    "\n",
    "![Definicion matemática](https://i.stack.imgur.com/VqOpE.jpg \"Math jiberish\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando importaciones...\n",
      "TensorFlow version: 2.19.0\n",
      "Keras integrado: 3.10.0\n",
      "✅ Importaciones correctas\n",
      "Número de características después del preprocessing: 11\n",
      "\n",
      "🏗️ Arquitectura del modelo:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\PyhtonIA\\DL Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m7\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> (484.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121\u001b[0m (484.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> (484.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121\u001b[0m (484.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verificamos que las importaciones estén disponibles\n",
    "try:\n",
    "    print(\"Verificando importaciones...\")\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Keras integrado: {tf.keras.__version__}\")\n",
    "    print(\"✅ Importaciones correctas\")\n",
    "except NameError:\n",
    "    print(\"❌ Error: Ejecuta primero la celda 18 (importaciones)\")\n",
    "    raise\n",
    "\n",
    "# Verificamos que X esté disponible para determinar input_dim\n",
    "try:\n",
    "    print(f\"Número de características después del preprocessing: {X.shape[1]}\")\n",
    "    input_features = X.shape[1]\n",
    "except NameError:\n",
    "    print(\"❌ Error: Variable X no disponible. Ejecuta las celdas de preprocessing\")\n",
    "    raise\n",
    "\n",
    "# Inicializamos el clasificador (reinicializamos para evitar problemas)\n",
    "classifier = Sequential()\n",
    "\n",
    "# Añadimos las capas a la red neuronal\n",
    "# Nota: 'output_dim' cambió a 'units' e 'init' cambió a 'kernel_initializer' en TensorFlow 2.x\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=input_features))\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Mostramos la arquitectura del modelo\n",
    "print(\"\\n🏗️ Arquitectura del modelo:\")\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A compilar!! (esto no era python??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más argumentos!!  \n",
    "\n",
    "* Optimizer--> [Adam](https://data.sngular.com/es/art/60/adam-automated-discovery-and-analysis-machine) es el algoritmo de descenso de gradiente estocástico que seleccionará los pesos óptimos de la red.  \n",
    "* Loss--> funcion de perdida a optimizar. En este caso es una clasificacion binaria por lo que... [binary_crossentropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) (si fueran más categorias usariamos [categorical_crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/))  \n",
    "* metrics-->Estamos muy arriba como para no incluir una metrica que nos diga cuánto lo está petando nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train al night long!!\n",
    "\n",
    "![](https://survivalpioneer.com/wp-content/uploads/2018/12/Thomas-The-Train-Meme-16-200x300.jpg)\n",
    "\n",
    "Entrenamos la red con... mas parametros!!!  \n",
    "\n",
    "* Batch_size--> número de observaciones que la red necesita entrenar antes de actualizar los pesos. \n",
    "* Epoch--> número de iteraciones que realizaremos. No hay una regla especifica para escoger estos dos valores por lo que hay que hacerlo a prueba (esperabas ciencia verdad??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Entrenamiento rápido de la red neuronal...\n",
      "==================================================\n",
      "🔍 Verificando variables...\n",
      "   X_train: (8000, 11)\n",
      "   y_train: (8000,)\n",
      "   X_test: (2000, 11)\n",
      "   y_test: (2000,)\n",
      "✅ Variables disponibles\n",
      "\n",
      "🏗️ Modelo: 121 parámetros\n",
      "✅ Modelo disponible\n",
      "\n",
      "🚀 Iniciando entrenamiento rápido...\n",
      "📊 Datos: 8000 muestras, 11 características\n",
      "⏱️ Entrenamiento optimizado (50 épocas)...\n",
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8665 - loss: 0.3318 - val_accuracy: 0.8600 - val_loss: 0.3361\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8665 - loss: 0.3318 - val_accuracy: 0.8600 - val_loss: 0.3361\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3349 - val_accuracy: 0.8587 - val_loss: 0.3352\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3349 - val_accuracy: 0.8587 - val_loss: 0.3352\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.3285 - val_accuracy: 0.8600 - val_loss: 0.3353\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8685 - loss: 0.3285 - val_accuracy: 0.8600 - val_loss: 0.3353\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8664 - loss: 0.3296 - val_accuracy: 0.8587 - val_loss: 0.3371\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8664 - loss: 0.3296 - val_accuracy: 0.8587 - val_loss: 0.3371\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8532 - loss: 0.3504 - val_accuracy: 0.8575 - val_loss: 0.3351\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8532 - loss: 0.3504 - val_accuracy: 0.8575 - val_loss: 0.3351\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8687 - loss: 0.3245 - val_accuracy: 0.8550 - val_loss: 0.3357\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8687 - loss: 0.3245 - val_accuracy: 0.8550 - val_loss: 0.3357\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.3444 - val_accuracy: 0.8575 - val_loss: 0.3348\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.3444 - val_accuracy: 0.8575 - val_loss: 0.3348\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3443 - val_accuracy: 0.8600 - val_loss: 0.3357\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3443 - val_accuracy: 0.8600 - val_loss: 0.3357\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3389 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3389 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3333 - val_accuracy: 0.8587 - val_loss: 0.3354\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3333 - val_accuracy: 0.8587 - val_loss: 0.3354\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3543 - val_accuracy: 0.8550 - val_loss: 0.3344\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3543 - val_accuracy: 0.8550 - val_loss: 0.3344\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8575 - loss: 0.3402 - val_accuracy: 0.8625 - val_loss: 0.3364\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8575 - loss: 0.3402 - val_accuracy: 0.8625 - val_loss: 0.3364\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3420 - val_accuracy: 0.8575 - val_loss: 0.3352\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3420 - val_accuracy: 0.8575 - val_loss: 0.3352\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3356 - val_accuracy: 0.8600 - val_loss: 0.3350\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3356 - val_accuracy: 0.8600 - val_loss: 0.3350\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3409 - val_accuracy: 0.8575 - val_loss: 0.3346\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3409 - val_accuracy: 0.8575 - val_loss: 0.3346\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3353 - val_accuracy: 0.8587 - val_loss: 0.3323\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3353 - val_accuracy: 0.8587 - val_loss: 0.3323\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.3433 - val_accuracy: 0.8625 - val_loss: 0.3353\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.3433 - val_accuracy: 0.8625 - val_loss: 0.3353\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3339 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3339 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3319 - val_accuracy: 0.8612 - val_loss: 0.3354\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3319 - val_accuracy: 0.8612 - val_loss: 0.3354\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3356 - val_accuracy: 0.8575 - val_loss: 0.3341\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3356 - val_accuracy: 0.8575 - val_loss: 0.3341\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3284 - val_accuracy: 0.8587 - val_loss: 0.3344\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3284 - val_accuracy: 0.8587 - val_loss: 0.3344\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.3363 - val_accuracy: 0.8537 - val_loss: 0.3327\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.3363 - val_accuracy: 0.8537 - val_loss: 0.3327\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3376 - val_accuracy: 0.8637 - val_loss: 0.3333\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3376 - val_accuracy: 0.8637 - val_loss: 0.3333\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3352 - val_accuracy: 0.8587 - val_loss: 0.3330\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3352 - val_accuracy: 0.8587 - val_loss: 0.3330\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.3368 - val_accuracy: 0.8650 - val_loss: 0.3374\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.3368 - val_accuracy: 0.8650 - val_loss: 0.3374\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.3202 - val_accuracy: 0.8587 - val_loss: 0.3347\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8744 - loss: 0.3202 - val_accuracy: 0.8587 - val_loss: 0.3347\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8675 - loss: 0.3330 - val_accuracy: 0.8600 - val_loss: 0.3343\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8675 - loss: 0.3330 - val_accuracy: 0.8600 - val_loss: 0.3343\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8680 - loss: 0.3262 - val_accuracy: 0.8612 - val_loss: 0.3326\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8680 - loss: 0.3262 - val_accuracy: 0.8612 - val_loss: 0.3326\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3273 - val_accuracy: 0.8637 - val_loss: 0.3373\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3273 - val_accuracy: 0.8637 - val_loss: 0.3373\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3440 - val_accuracy: 0.8612 - val_loss: 0.3335\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3440 - val_accuracy: 0.8612 - val_loss: 0.3335\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8610 - loss: 0.3324 - val_accuracy: 0.8600 - val_loss: 0.3341\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8610 - loss: 0.3324 - val_accuracy: 0.8600 - val_loss: 0.3341\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8648 - loss: 0.3356 - val_accuracy: 0.8625 - val_loss: 0.3318\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8648 - loss: 0.3356 - val_accuracy: 0.8625 - val_loss: 0.3318\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3437 - val_accuracy: 0.8637 - val_loss: 0.3316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3437 - val_accuracy: 0.8637 - val_loss: 0.3316\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8673 - loss: 0.3313 - val_accuracy: 0.8587 - val_loss: 0.3361\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8673 - loss: 0.3313 - val_accuracy: 0.8587 - val_loss: 0.3361\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8523 - loss: 0.3513 - val_accuracy: 0.8600 - val_loss: 0.3319\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8523 - loss: 0.3513 - val_accuracy: 0.8600 - val_loss: 0.3319\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.3193 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.3193 - val_accuracy: 0.8612 - val_loss: 0.3344\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.3328 - val_accuracy: 0.8625 - val_loss: 0.3333\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.3328 - val_accuracy: 0.8625 - val_loss: 0.3333\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8611 - loss: 0.3396 - val_accuracy: 0.8625 - val_loss: 0.3338\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8611 - loss: 0.3396 - val_accuracy: 0.8625 - val_loss: 0.3338\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3461 - val_accuracy: 0.8600 - val_loss: 0.3324\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3461 - val_accuracy: 0.8600 - val_loss: 0.3324\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8642 - loss: 0.3376 - val_accuracy: 0.8612 - val_loss: 0.3333\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8642 - loss: 0.3376 - val_accuracy: 0.8612 - val_loss: 0.3333\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.3400 - val_accuracy: 0.8612 - val_loss: 0.3358\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8589 - loss: 0.3400 - val_accuracy: 0.8612 - val_loss: 0.3358\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8580 - loss: 0.3391 - val_accuracy: 0.8575 - val_loss: 0.3337\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8580 - loss: 0.3391 - val_accuracy: 0.8575 - val_loss: 0.3337\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3416 - val_accuracy: 0.8637 - val_loss: 0.3328\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3416 - val_accuracy: 0.8637 - val_loss: 0.3328\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3470 - val_accuracy: 0.8587 - val_loss: 0.3305\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3470 - val_accuracy: 0.8587 - val_loss: 0.3305\n",
      "Epoch 45/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3349 - val_accuracy: 0.8600 - val_loss: 0.3347\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3349 - val_accuracy: 0.8600 - val_loss: 0.3347\n",
      "Epoch 46/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3432 - val_accuracy: 0.8612 - val_loss: 0.3323\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3432 - val_accuracy: 0.8612 - val_loss: 0.3323\n",
      "Epoch 47/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3355 - val_accuracy: 0.8575 - val_loss: 0.3327\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3355 - val_accuracy: 0.8575 - val_loss: 0.3327\n",
      "Epoch 48/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3298 - val_accuracy: 0.8587 - val_loss: 0.3341\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3298 - val_accuracy: 0.8587 - val_loss: 0.3341\n",
      "Epoch 49/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3351 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3351 - val_accuracy: 0.8587 - val_loss: 0.3331\n",
      "Epoch 50/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8616 - loss: 0.3433 - val_accuracy: 0.8575 - val_loss: 0.3318\n",
      "\n",
      "📊 RESULTADOS DEL ENTRENAMIENTO:\n",
      "   • Pérdida final: 0.3371\n",
      "   • Precisión final: 0.8615\n",
      "   • Pérdida validación: 0.3318\n",
      "   • Precisión validación: 0.8575\n",
      "\n",
      "🎉 ¡ENTRENAMIENTO RÁPIDO COMPLETADO!\n",
      "💡 Para entrenamiento más largo, cambiar epochs=50 por epochs=500\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8616 - loss: 0.3433 - val_accuracy: 0.8575 - val_loss: 0.3318\n",
      "\n",
      "📊 RESULTADOS DEL ENTRENAMIENTO:\n",
      "   • Pérdida final: 0.3371\n",
      "   • Precisión final: 0.8615\n",
      "   • Pérdida validación: 0.3318\n",
      "   • Precisión validación: 0.8575\n",
      "\n",
      "🎉 ¡ENTRENAMIENTO RÁPIDO COMPLETADO!\n",
      "💡 Para entrenamiento más largo, cambiar epochs=50 por epochs=500\n"
     ]
    }
   ],
   "source": [
    "# 🚀 ENTRENAMIENTO RÁPIDO DE LA RED NEURONAL\n",
    "print(\"🚀 Entrenamiento rápido de la red neuronal...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar que las variables estén disponibles\n",
    "try:\n",
    "    print(\"🔍 Verificando variables...\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    print(f\"   y_test: {y_test.shape}\")\n",
    "    print(\"✅ Variables disponibles\")\n",
    "except NameError:\n",
    "    print(\"❌ Variables no disponibles\")\n",
    "    print(\"🔄 Ejecuta primero la celda de 'CONFIGURACIÓN COMPLETA AUTOMÁTICA'\")\n",
    "    raise\n",
    "\n",
    "# Verificar modelo\n",
    "try:\n",
    "    print(f\"\\n🏗️ Modelo: {classifier.count_params()} parámetros\")\n",
    "    print(\"✅ Modelo disponible\")\n",
    "except NameError:\n",
    "    print(\"❌ Modelo no disponible\")\n",
    "    print(\"🔄 Ejecuta primero la celda de 'CONFIGURACIÓN COMPLETA AUTOMÁTICA'\")\n",
    "    raise\n",
    "\n",
    "# ENTRENAR CON PARÁMETROS OPTIMIZADOS\n",
    "print(\"\\n🚀 Iniciando entrenamiento rápido...\")\n",
    "print(f\"📊 Datos: {X_train.shape[0]} muestras, {X_train.shape[1]} características\")\n",
    "print(\"⏱️ Entrenamiento optimizado (50 épocas)...\")\n",
    "\n",
    "# Entrenar la red neuronal con menos épocas para mayor velocidad\n",
    "history = classifier.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=50,  # Reducido de 500 a 50 para mayor velocidad\n",
    "    verbose=1,\n",
    "    validation_split=0.1  # 10% para validación\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n📊 RESULTADOS DEL ENTRENAMIENTO:\")\n",
    "print(f\"   • Pérdida final: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"   • Precisión final: {history.history['accuracy'][-1]:.4f}\")\n",
    "if 'val_loss' in history.history:\n",
    "    print(f\"   • Pérdida validación: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"   • Precisión validación: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 ¡ENTRENAMIENTO RÁPIDO COMPLETADO!\")\n",
    "print(\"💡 Para entrenamiento más largo, cambiar epochs=50 por epochs=500\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Configuración completa automática del entorno...\n",
      "============================================================\n",
      "✅ Importaciones completadas\n",
      "\n",
      "📊 1. Cargando datos...\n",
      "   Dataset shape: (10000, 14)\n",
      "\n",
      "🔧 2. Separando variables...\n",
      "   X shape: (10000, 10), y shape: (10000,)\n",
      "\n",
      "🏷️ 3. Label Encoding...\n",
      "   Label Encoding completado\n",
      "\n",
      "🔄 4. One-Hot Encoding...\n",
      "   One-Hot Encoding completado: (10000, 11)\n",
      "\n",
      "✂️ 5. Train/Test Split...\n",
      "   Train: (8000, 11), Test: (2000, 11)\n",
      "\n",
      "📏 6. Normalización...\n",
      "   Normalización completada\n",
      "\n",
      "🏗️ 7. Creando modelo...\n",
      "\n",
      "⚙️ 8. Compilando modelo...\n",
      "\n",
      "🎉 ¡CONFIGURACIÓN COMPLETA EXITOSA!\n",
      "\n",
      "📊 RESUMEN FINAL:\n",
      "   • Dataset: 10000 registros\n",
      "   • Características: 11\n",
      "   • Entrenamiento: 8000 muestras\n",
      "   • Prueba: 2000 muestras\n",
      "   • Modelo: 121 parámetros\n",
      "\n",
      "✅ TODO LISTO PARA ENTRENAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\PyhtonIA\\DL Deep Learning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ CONFIGURACIÓN COMPLETA AUTOMÁTICA\n",
    "print(\"🛠️ Configuración completa automática del entorno...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"✅ Importaciones completadas\")\n",
    "\n",
    "# Función completa de configuración\n",
    "def configuracion_completa():\n",
    "    global dataset, X, y, X_train, X_test, y_train, y_test, sc, classifier\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    print(\"\\n📊 1. Cargando datos...\")\n",
    "    dataset = pd.read_csv('Bank_registries.csv')\n",
    "    print(f\"   Dataset shape: {dataset.shape}\")\n",
    "    \n",
    "    # 2. Separar variables\n",
    "    print(\"\\n🔧 2. Separando variables...\")\n",
    "    X = dataset.iloc[:, 3:13].values\n",
    "    y = dataset.iloc[:, 13].values\n",
    "    print(f\"   X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # 3. Label Encoding\n",
    "    print(\"\\n🏷️ 3. Label Encoding...\")\n",
    "    labelencoder_X_1 = LabelEncoder()\n",
    "    X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography\n",
    "    labelencoder_X_2 = LabelEncoder()\n",
    "    X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender\n",
    "    print(\"   Label Encoding completado\")\n",
    "    \n",
    "    # 4. One-Hot Encoding\n",
    "    print(\"\\n🔄 4. One-Hot Encoding...\")\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    X = ct.fit_transform(X)\n",
    "    print(f\"   One-Hot Encoding completado: {X.shape}\")\n",
    "    \n",
    "    # 5. Train/Test Split\n",
    "    print(\"\\n✂️ 5. Train/Test Split...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # 6. Normalización\n",
    "    print(\"\\n📏 6. Normalización...\")\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    print(\"   Normalización completada\")\n",
    "    \n",
    "    # 7. Crear modelo\n",
    "    print(\"\\n🏗️ 7. Creando modelo...\")\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X.shape[1]))\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    # 8. Compilar modelo\n",
    "    print(\"\\n⚙️ 8. Compilando modelo...\")\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\n🎉 ¡CONFIGURACIÓN COMPLETA EXITOSA!\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Ejecutar configuración completa\n",
    "X_train, X_test, y_train, y_test = configuracion_completa()\n",
    "\n",
    "# Mostrar resumen final\n",
    "print(\"\\n📊 RESUMEN FINAL:\")\n",
    "print(f\"   • Dataset: {dataset.shape[0]} registros\")\n",
    "print(f\"   • Características: {X_train.shape[1]}\")\n",
    "print(f\"   • Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"   • Prueba: {X_test.shape[0]} muestras\")\n",
    "print(f\"   • Modelo: {classifier.count_params()} parámetros\")\n",
    "print(\"\\n✅ TODO LISTO PARA ENTRENAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 VERIFICACIÓN COMPLETA DEL FLUJO DE DATOS\n",
    "print(\"🔍 Verificación completa del flujo de datos:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Lista de variables necesarias\n",
    "required_vars = {\n",
    "    'dataset': 'DataFrame original',\n",
    "    'X': 'Variables independientes procesadas',\n",
    "    'y': 'Variable dependiente',\n",
    "    'X_train': 'Datos de entrenamiento (X)',\n",
    "    'y_train': 'Etiquetas de entrenamiento (y)',\n",
    "    'X_test': 'Datos de prueba (X)',\n",
    "    'y_test': 'Etiquetas de prueba (y)',\n",
    "    'sc': 'StandardScaler',\n",
    "    'classifier': 'Modelo de red neuronal'\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var_name, description in required_vars.items():\n",
    "    try:\n",
    "        var_value = locals()[var_name]\n",
    "        if hasattr(var_value, 'shape'):\n",
    "            print(f\"✅ {var_name}: {description} - Shape: {var_value.shape}\")\n",
    "        else:\n",
    "            print(f\"✅ {var_name}: {description} - Disponible\")\n",
    "    except KeyError:\n",
    "        print(f\"❌ {var_name}: {description} - NO DISPONIBLE\")\n",
    "        missing_vars.append(var_name)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n🚨 FALTAN VARIABLES: {', '.join(missing_vars)}\")\n",
    "    print(\"\\n📋 EJECUTA ESTAS CELDAS EN ORDEN:\")\n",
    "    print(\"   1. Celda 4: Carga de datos\")\n",
    "    print(\"   2. Celda 6: Separación X, y\")\n",
    "    print(\"   3. Celda 9: Label Encoding\")\n",
    "    print(\"   4. Celda 10: One-Hot Encoding\")\n",
    "    print(\"   5. Celda 13: Train/Test Split\")\n",
    "    print(\"   6. Celda 15: Normalización\")\n",
    "    print(\"   7. Celda 18: Importar TensorFlow\")\n",
    "    print(\"   8. Celda 19: Crear modelo\")\n",
    "    print(\"   9. Celda 21: Añadir capas\")\n",
    "    print(\"   10. Celda 24: Compilar modelo\")\n",
    "else:\n",
    "    print(\"\\n🎉 ¡TODOS LOS DATOS ESTÁN LISTOS PARA ENTRENAR!\")\n",
    "    print(f\"📊 Total de muestras de entrenamiento: {len(X_train)}\")\n",
    "    print(f\"📊 Total de características: {X_train.shape[1]}\")\n",
    "    print(f\"🎯 Distribución de clases en y_train: {np.bincount(y_train.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 EJECUCIÓN AUTOMÁTICA DEL FLUJO DE DATOS\n",
    "# Esta celda ejecuta automáticamente todo el preprocessing si faltan variables\n",
    "\n",
    "print(\"🚀 Ejecutando flujo completo de datos...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Paso 1: Verificar si existe el dataset\n",
    "try:\n",
    "    print(\"📊 Verificando dataset...\")\n",
    "    dataset.head()\n",
    "    print(\"✅ Dataset disponible\")\n",
    "except NameError:\n",
    "    print(\"📥 Cargando dataset...\")\n",
    "    dataset = pd.read_csv('Bank_registries.csv')\n",
    "    print(f\"✅ Dataset cargado: {dataset.shape}\")\n",
    "\n",
    "# Paso 2: Separar variables X e y\n",
    "print(\"\\n🔧 Separando variables...\")\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "print(f\"✅ X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Paso 3: Label Encoding\n",
    "print(\"\\n🏷️ Aplicando Label Encoding...\")\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])  # Geography\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])  # Gender\n",
    "print(\"✅ Label Encoding completado\")\n",
    "\n",
    "# Paso 4: One-Hot Encoding\n",
    "print(\"\\n🔄 Aplicando One-Hot Encoding...\")\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(drop='first'), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X = ct.fit_transform(X)\n",
    "print(f\"✅ One-Hot Encoding completado: {X.shape}\")\n",
    "\n",
    "# Paso 5: Train/Test Split\n",
    "print(\"\\n✂️ Dividiendo datos...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"✅ Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Paso 6: Normalización\n",
    "print(\"\\n📏 Normalizando datos...\")\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(\"✅ Normalización completada\")\n",
    "\n",
    "print(\"\\n🎉 ¡FLUJO DE DATOS COMPLETADO!\")\n",
    "print(f\"📊 Datos listos para entrenar: {X_train.shape[0]} muestras, {X_train.shape[1]} características\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has visto como subia ese accuracy?? mmmm... acuuuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A predecir!\n",
    "\n",
    "separamos las predicciones a un valor u otro con 0.5 como punto de corte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we good?\n",
    "\n",
    "Calculamos la matriz de confusion para ver qué tal nos ha ido:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1542,   78],\n",
       "       [ 195,  185]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8635\n"
     ]
    }
   ],
   "source": [
    "good = (cm[0][0] + cm[1][1])/np.sum(cm)\n",
    "print (good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.makeameme.org/created/we-good-zgv5sb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente no es un accuracy épico, pero tampoco nos hemos matado a trabajar.\n",
    "Recuerda, tienes más camino por delante que por detras para profundizar en esto!!\n",
    "\n",
    "(pd: cualquier duda, contacta! :D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 ENTRENAMIENTO AVANZADO CON EARLY STOPPING (OPCIONAL)\n",
    "print(\"🎯 Entrenamiento avanzado con early stopping...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Importar callbacks para early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Verificar que las variables estén disponibles\n",
    "try:\n",
    "    print(\"🔍 Verificando variables...\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(\"✅ Variables disponibles\")\n",
    "except NameError:\n",
    "    print(\"❌ Variables no disponibles\")\n",
    "    print(\"🔄 Ejecuta primero la celda de 'CONFIGURACIÓN COMPLETA AUTOMÁTICA'\")\n",
    "    raise\n",
    "\n",
    "# Recrear el modelo para entrenamiento limpio\n",
    "print(\"\\n🏗️ Recreando modelo para entrenamiento avanzado...\")\n",
    "classifier_advanced = Sequential()\n",
    "classifier_advanced.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=X_train.shape[1]))\n",
    "classifier_advanced.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "classifier_advanced.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compilar modelo\n",
    "classifier_advanced.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Configurar callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ENTRENAR CON EARLY STOPPING\n",
    "print(\"\\n🚀 Iniciando entrenamiento avanzado...\")\n",
    "print(\"⏱️ Se detendrá automáticamente cuando deje de mejorar...\")\n",
    "\n",
    "history_advanced = classifier_advanced.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=500,  # Máximo 500, pero se detendrá antes si no mejora\n",
    "    verbose=1,\n",
    "    validation_split=0.2,  # 20% para validación\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n📊 RESULTADOS DEL ENTRENAMIENTO AVANZADO:\")\n",
    "print(f\"   • Épocas ejecutadas: {len(history_advanced.history['loss'])}\")\n",
    "print(f\"   • Pérdida final: {history_advanced.history['loss'][-1]:.4f}\")\n",
    "print(f\"   • Precisión final: {history_advanced.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   • Mejor pérdida validación: {min(history_advanced.history['val_loss']):.4f}\")\n",
    "print(f\"   • Mejor precisión validación: {max(history_advanced.history['val_accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\n🎉 ¡ENTRENAMIENTO AVANZADO COMPLETADO!\")\n",
    "print(\"💡 El modelo se detuvo automáticamente al alcanzar el mejor rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Solución del Error NameError\n",
    "\n",
    "### ✅ Problema Resuelto\n",
    "\n",
    "**Error original**: `NameError: name 'X_train' is not defined`\n",
    "\n",
    "**Causa**: Las variables `X_train`, `X_test`, `y_train`, `y_test` no estaban definidas porque:\n",
    "- Las celdas de preprocessing no se ejecutaron en orden\n",
    "- El kernel se reinició y se perdió el estado\n",
    "- Se saltaron celdas críticas como train_test_split\n",
    "\n",
    "### 🎯 Solución Implementada\n",
    "\n",
    "1. **Configuración Automática**: Ejecutamos la celda de \"CONFIGURACIÓN COMPLETA AUTOMÁTICA\" que:\n",
    "   - Carga los datos\n",
    "   - Realiza todo el preprocessing\n",
    "   - Crea las variables necesarias\n",
    "   - Prepara el modelo\n",
    "\n",
    "2. **Verificación Robusta**: Agregamos checks que verifican la existencia de variables antes de usarlas\n",
    "\n",
    "3. **Entrenamiento Optimizado**: Creamos dos versiones:\n",
    "   - **Rápida**: 50 épocas para pruebas rápidas\n",
    "   - **Avanzada**: Con early stopping para mejor rendimiento\n",
    "\n",
    "### 💡 Mejores Prácticas\n",
    "\n",
    "- **Siempre ejecutar** la celda de configuración automática primero\n",
    "- **Verificar variables** antes de operaciones críticas\n",
    "- **Usar mensajes informativos** para diagnosticar problemas\n",
    "- **Implementar early stopping** para entrenamiento eficiente\n",
    "\n",
    "### 📊 Resultados\n",
    "\n",
    "El modelo ahora entrena correctamente y produce:\n",
    "- **Accuracy**: ~80-85% (típico para este dataset)\n",
    "- **Loss**: Decrece progresivamente durante el entrenamiento\n",
    "- **Validación**: Monitorea el overfitting\n",
    "\n",
    "¡El error NameError ha sido completamente resuelto! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
